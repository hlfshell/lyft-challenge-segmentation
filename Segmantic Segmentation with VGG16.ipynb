{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmantic Segmentation with VGG16\n",
    "\n",
    "The initial attempt with using a GAN hit a wall, specifically with figuring out how to do proper layer sizing for the transposed convolution layers for the generator. On top of that, some research I did during my \"off\" time showed that the approach semantic segmentation using either an FCN (Fully Convolutional Network) approach or SegNet would work best.\n",
    "\n",
    "Since SegNet is completely new, whereas transfer learning with VGG16 is something I'm familiar with, I'm going to attempt a VGG16 network here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n",
      "TensorFlow Version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from glob import glob\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "#Check GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful variables\n",
    "num_classes = 13 # none and 12 options, 0-12\n",
    "image_shape = (160, 576)\n",
    "weights_initializer_stddev = 0.01\n",
    "weights_regularized_l2 = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16\n",
    "\n",
    "\n",
    "First we're going to load VGG16 with pretrained weights (so it maintains its feature detectors, which can be useful for our smaller dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "#Download VGG16 if it is not already\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"vgg16.zip\"):\n",
    "    urlretrieve(\n",
    "        'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip',\n",
    "        \"./vgg16.zip\")\n",
    "    print(\"Downloaded VGG16 model weights\")\n",
    "else:\n",
    "    print(\"Already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping extraction\n"
     ]
    }
   ],
   "source": [
    "#Extract if needed\n",
    "if not os.path.exists(\"./vgg\"):\n",
    "    unzip = zipfile.ZipFile(\"./vgg16.zip\", \"r\")\n",
    "    unzip.extractall(\"./\")\n",
    "    unzip.close()\n",
    "    print(\"Extracted VGG16 model weights\")\n",
    "else:\n",
    "    print(\"Already exists, skipping extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a map more colorful\n",
    "def convertToColor(value):\n",
    "    colors = [\n",
    "        (255, 255, 255),   #0\n",
    "        (255, 0, 0),      #1\n",
    "        (0, 255, 0),      #2\n",
    "        (0, 0, 255),      #3\n",
    "        (255, 255, 0),    #4\n",
    "        (127, 0, 255),    #5\n",
    "        (51, 255, 51),    #6\n",
    "        (255, 0, 127),    #7\n",
    "        (127, 127, 127),  #8\n",
    "        (0, 0, 0),        #9\n",
    "        (0, 255, 255),  #10\n",
    "        (0, 0, 100),      #11\n",
    "        (100, 0, 0),      #12\n",
    "    ]\n",
    "    return colors[value[0]]\n",
    "\n",
    "def colorizeMap(img):\n",
    "    return [list( map(convertToColor, row) ) for row in img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to be able to convert the image map to a 13 channel ground truth map, and vice versa\n",
    "def pixelToTruth(value):\n",
    "    truths = [\n",
    "        (1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)\n",
    "    ]\n",
    "    \n",
    "    return truths[value[0]]\n",
    "\n",
    "def truthToPixel(value):\n",
    "    return (value.tolist().index(1), 0, 0)\n",
    "\n",
    "def imageToTruth(img):\n",
    "    return [list(map(pixelToTruth, row)) for row in img]\n",
    "\n",
    "def truthToImage(truth):\n",
    "    return [list(map(truthToPixel, row)) for row in truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the image data and the label for it\n",
    "def get_training_data(batch_size):\n",
    "    #Both inputs and ground truth maps have the same name - easy!\n",
    "    image_paths = glob(os.path.join(\"./data/Train/CameraRGB\", \"*.png\"))\n",
    "    label_paths = glob(os.path.join(\"./data/Train/CameraSeg\", \"*.png\"))\n",
    "    \n",
    "    for batch in range(0, len(image_paths), batch_size):\n",
    "        images = []\n",
    "        maps = []\n",
    "    \n",
    "    for batch in range(0, len(image_paths), batch_size):\n",
    "        images = []\n",
    "        maps = []\n",
    "        \n",
    "        for index, image_file in enumerate(image_paths[batch:batch + batch_size]):\n",
    "            map_file = os.path.join(label_paths[index])\n",
    "            \n",
    "            image = cv2.imread(image_file)\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, image_shape)\n",
    "#             image = image - np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "            map_image = cv2.imread(map_file)\n",
    "            map_image = cv2.cvtColor(map_image, cv2.COLOR_BGR2RGB)\n",
    "            map_image = cv2.resize(map_image, image_shape)\n",
    "            map_image = imageToTruth(map_image)\n",
    "            \n",
    "            images.append(image)\n",
    "            maps.append(map_image)\n",
    "\n",
    "        yield np.array(images), np.array(maps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_placeholders():\n",
    "    #Placeholders\n",
    "    label = tf.placeholder(tf.int32, (None, None, None, num_classes), name='label')\n",
    "    learning_rate = tf.placeholder(tf.int32, name='learning_rate')\n",
    "    \n",
    "    return label, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg():\n",
    "    #Grab layers from pretrained VGG\n",
    "    tf.saved_model.loader.load(sess, [\"vgg16\"], \"./vgg/\")\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #define key layers for us to work with, so we can take pieces of VGG16\n",
    "    #for our own use\n",
    "    input_layer = graph.get_tensor_by_name(\"image_input:0\")\n",
    "    keep_prob = graph.get_tensor_by_name(\"keep_prob:0\") #Dropout settings\n",
    "    \n",
    "    #more layer grabbing\n",
    "    layer_3 = graph.get_tensor_by_name(\"layer3_out:0\")\n",
    "    layer_4 = graph.get_tensor_by_name(\"layer4_out:0\")\n",
    "    layer_7 = graph.get_tensor_by_name(\"layer7_out:0\")\n",
    "    \n",
    "    return graph, input_layer, keep_prob, layer_3, layer_4, layer_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_fcn(graph, input_layer, keep_prob, layer_3, layer_4, layer_7):\n",
    "    # Skip connections for later\n",
    "    skip_conv_3 = tf.layers.conv2d(layer_3, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    skip_conv_4 = tf.layers.conv2d(layer_4, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #Layer 7 isn't skipped, it's passed right to transpose    \n",
    "    fully_connected_convs = tf.layers.conv2d(layer_7, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #From layer 7 we need to transpose up\n",
    "    transpose_1 = tf.layers.conv2d_transpose(fully_connected_convs, num_classes, 4, 2, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    # Add the skip layer from layer 4\n",
    "    skip_1 = tf.add(transpose_1, skip_conv_4)\n",
    "    \n",
    "    #Tranpose up from resultant layer\n",
    "    transpose_2 = tf.layers.conv2d_transpose(skip_1, num_classes, 4, 2, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #Create skip layer from layer 3\n",
    "    skip_2 = tf.add(skip_conv_3, transpose_2)\n",
    "    \n",
    "    #Final output layer\n",
    "    output_layer = tf.layers.conv2d_transpose(skip_2, num_classes, 16, 8, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "            activation=tf.sigmoid)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(label, output_layer):\n",
    "    #Create the optimzer\n",
    "    logits = tf.reshape(output_layer, (-1, num_classes))\n",
    "    correct_label = tf.reshape(label, (-1, num_classes))\n",
    "    \n",
    "    return logits, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(logits, correct_labels):\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_labels))\n",
    "    optimizer= tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy_loss)\n",
    "    return cross_entropy_loss, optimizer, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 10\n",
    "keep_probability = 0.5 \n",
    "learning_rate_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, input_layer, label, keep_prob, learning_rate):\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Graph has been built - launching training\")\n",
    "    print(\"====== :-) ======\")\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Launching Epoch {}\".format(epoch))\n",
    "        loss_log = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        #get the images\n",
    "        for image, truth in get_training_data(batch_size):\n",
    "            batch_count += 1\n",
    "            _, loss = sess.run(\n",
    "                    [train_op, cross_entropy_loss],\n",
    "                    feed_dict = {\n",
    "                        input_layer: image,\n",
    "                        label: truth,\n",
    "                        keep_prob: keep_probability,\n",
    "                        learning_rate: learning_rate_alpha\n",
    "                    }\n",
    "                )\n",
    "            loss_log.append('{:3f}'.format(loss))\n",
    "            if(batch_count % 10 == 0):\n",
    "                print(\"Batch {} - loss of {}\".format(batch_count, loss))\n",
    "        print(\"Training for epoch finished - \", loss_log[-1])\n",
    "        print()\n",
    "    print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_on_image(sess, logits, input_layer, label, keep_prob, learning_rate):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    chosen_image = \"165\"\n",
    "    image_file = \"./data/Train/CameraRGB/{}.png\".format(chosen_image)\n",
    "    truth_file = \"./data/Train/CameraSeg/{}.png\".format(chosen_image)\n",
    "    \n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.resize(image, image_shape)\n",
    "    \n",
    "    truth = cv2.imread(truth_file)\n",
    "    truth = cv2.cvtColor(truth, cv2.COLOR_BGR2RGB)\n",
    "    truth = cv2.resize(truth, image_shape)\n",
    "    truth = imageToTruth(truth)\n",
    "    \n",
    "    return sess.run([output_layer], feed_dict={\n",
    "            input_layer: np.array([image]),\n",
    "            label: np.array([truth]),\n",
    "            keep_prob: keep_probability,\n",
    "            learning_rate: learning_rate_alpha\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./vgg/variables/variables'\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    label, learning_rate = get_placeholders()\n",
    "    \n",
    "    graph, input_layer, keep_prob, layer_3, layer_4, layer_7 = load_vgg()\n",
    "    \n",
    "    output_layer = vgg_fcn(graph, input_layer, keep_prob, layer_3, layer_4, layer_7)\n",
    "\n",
    "    logits, correct_labels = get_logits(label, output_layer)\n",
    "    \n",
    "    cross_entropy_loss, optimizer, train_op = get_loss(logits, correct_labels)\n",
    "\n",
    "    #and now, training!\n",
    "#     train(sess, input_layer, label, keep_prob, learning_rate)\n",
    "    \n",
    "    #execute graph on a singular image\n",
    "    output_test = execute_on_image(sess, logits, input_layer, label, keep_prob, learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 160, 13)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7.67056823e-01, 3.09352130e-01, 1.31825760e-01, ...,\n",
       "         2.91122705e-01, 1.60486817e-01, 7.48600543e-01],\n",
       "        [8.68503228e-02, 1.86929703e-01, 5.14724627e-02, ...,\n",
       "         9.99738872e-01, 2.25904211e-03, 3.45170265e-04],\n",
       "        [9.98210669e-01, 1.14997312e-01, 5.11900435e-05, ...,\n",
       "         9.99963164e-01, 9.93969083e-01, 8.65127367e-04],\n",
       "        ...,\n",
       "        [5.74969769e-01, 4.77827191e-01, 2.98987150e-01, ...,\n",
       "         1.57196954e-01, 9.02837217e-01, 1.33709073e-01],\n",
       "        [7.45507777e-01, 7.63445735e-01, 6.01666272e-01, ...,\n",
       "         7.19402909e-01, 5.16307831e-01, 8.78918827e-01],\n",
       "        [2.98375100e-01, 7.04606473e-01, 3.46926153e-01, ...,\n",
       "         8.81381750e-01, 1.81084663e-01, 6.07959151e-01]],\n",
       "\n",
       "       [[9.99996424e-01, 9.15998518e-01, 2.99819499e-01, ...,\n",
       "         8.07579815e-01, 9.75729823e-01, 1.46905193e-03],\n",
       "        [1.01733254e-02, 4.47796099e-03, 5.23329824e-02, ...,\n",
       "         9.98283386e-01, 9.98081923e-01, 9.81153250e-01],\n",
       "        [6.45237088e-01, 9.99958277e-01, 7.49354303e-01, ...,\n",
       "         9.98909950e-01, 9.82536554e-01, 9.97001827e-01],\n",
       "        ...,\n",
       "        [7.38901854e-01, 5.76569200e-01, 1.97195515e-01, ...,\n",
       "         3.06770414e-01, 2.89311320e-01, 4.29610848e-01],\n",
       "        [4.26049471e-01, 9.16268826e-01, 5.16890526e-01, ...,\n",
       "         5.03565252e-01, 3.97897810e-01, 5.95658600e-01],\n",
       "        [3.08055878e-01, 6.44134820e-01, 3.62679094e-01, ...,\n",
       "         6.19884133e-01, 6.49683416e-01, 2.59905398e-01]],\n",
       "\n",
       "       [[4.96385008e-01, 7.90371895e-01, 1.13599366e-02, ...,\n",
       "         1.71941027e-01, 5.45929857e-02, 8.11703503e-01],\n",
       "        [9.55729187e-01, 2.59068102e-01, 6.22286415e-03, ...,\n",
       "         1.62193738e-02, 4.12776440e-01, 9.92377043e-01],\n",
       "        [9.62395608e-01, 3.32045972e-01, 3.84372615e-05, ...,\n",
       "         8.66348203e-03, 8.31960440e-01, 3.58016007e-02],\n",
       "        ...,\n",
       "        [3.54254514e-01, 3.46503317e-01, 2.73319185e-01, ...,\n",
       "         8.54062021e-01, 4.67859745e-01, 5.48231125e-01],\n",
       "        [2.31669441e-01, 3.22368681e-01, 3.52699399e-01, ...,\n",
       "         6.63776577e-01, 6.15737736e-01, 7.24985838e-01],\n",
       "        [5.44653952e-01, 3.04479241e-01, 7.13186920e-01, ...,\n",
       "         6.01417959e-01, 3.08085024e-01, 4.95515823e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.72134244e-01, 3.94038036e-02, 8.83069396e-01, ...,\n",
       "         1.98672950e-01, 2.46924744e-03, 2.05147654e-01],\n",
       "        [3.67045701e-01, 1.94758538e-03, 6.49371684e-01, ...,\n",
       "         9.97802317e-01, 1.92832381e-01, 3.51840109e-01],\n",
       "        [9.56746101e-01, 8.60012311e-04, 8.81876349e-01, ...,\n",
       "         3.36408585e-01, 4.03594911e-01, 6.80391816e-03],\n",
       "        ...,\n",
       "        [6.04712069e-01, 7.36889899e-01, 1.34688814e-03, ...,\n",
       "         8.63294959e-01, 9.77147937e-01, 9.97840524e-01],\n",
       "        [1.40951219e-04, 9.40025508e-01, 6.16088137e-02, ...,\n",
       "         9.84275281e-01, 6.75089419e-01, 9.03231025e-01],\n",
       "        [9.37783360e-01, 1.12686224e-01, 5.79544067e-01, ...,\n",
       "         9.65473831e-01, 8.85893226e-01, 9.99759972e-01]],\n",
       "\n",
       "       [[8.39809477e-02, 9.98035014e-01, 9.19874609e-01, ...,\n",
       "         7.90360391e-01, 9.32942424e-03, 8.83708477e-01],\n",
       "        [9.62095737e-01, 2.24412954e-03, 9.42993164e-01, ...,\n",
       "         1.18629076e-03, 8.79474953e-02, 2.74549723e-01],\n",
       "        [2.98405136e-03, 8.19441617e-01, 7.24502206e-01, ...,\n",
       "         9.83320832e-01, 9.52613711e-01, 7.57417798e-01],\n",
       "        ...,\n",
       "        [9.74347651e-01, 3.96832684e-03, 8.72677386e-01, ...,\n",
       "         9.99958277e-01, 2.01671906e-02, 1.04151398e-01],\n",
       "        [3.60234790e-02, 3.27328831e-01, 6.84720099e-01, ...,\n",
       "         8.81937325e-01, 9.97203946e-01, 5.93290269e-01],\n",
       "        [8.38998139e-01, 9.78953660e-01, 9.69695747e-01, ...,\n",
       "         3.19885373e-01, 9.85516012e-01, 7.79053709e-03]],\n",
       "\n",
       "       [[4.13569719e-01, 9.61568236e-01, 3.70962083e-01, ...,\n",
       "         1.12280799e-02, 9.92692053e-01, 4.34198260e-01],\n",
       "        [9.06795382e-01, 8.60379577e-01, 9.60747004e-01, ...,\n",
       "         1.33685740e-02, 3.38454366e-01, 9.97545660e-01],\n",
       "        [8.96685064e-01, 3.96203518e-01, 9.58063304e-01, ...,\n",
       "         9.48554397e-01, 7.67331943e-02, 8.90479088e-01],\n",
       "        ...,\n",
       "        [3.43185067e-02, 8.54498744e-01, 9.95469213e-01, ...,\n",
       "         9.40006614e-01, 8.08785796e-01, 2.86350362e-02],\n",
       "        [9.99739468e-01, 1.56411260e-01, 9.17107999e-01, ...,\n",
       "         3.84743035e-01, 1.82809487e-01, 9.46036279e-01],\n",
       "        [6.13554239e-01, 9.95211780e-01, 2.93902278e-01, ...,\n",
       "         9.98663664e-01, 9.83220041e-01, 4.86976475e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92160"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "160*576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
