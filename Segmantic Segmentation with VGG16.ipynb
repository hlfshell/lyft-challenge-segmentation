{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmantic Segmentation with VGG16\n",
    "\n",
    "The initial attempt with using a GAN hit a wall, specifically with figuring out how to do proper layer sizing for the transposed convolution layers for the generator. On top of that, some research I did during my \"off\" time showed that the approach semantic segmentation using either an FCN (Fully Convolutional Network) approach or SegNet would work best.\n",
    "\n",
    "Since SegNet is completely new, whereas transfer learning with VGG16 is something I'm familiar with, I'm going to attempt a VGG16 network here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n",
      "TensorFlow Version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from glob import glob\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "#Check GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful variables\n",
    "num_classes = 13 # none and 12 options, 0-12\n",
    "image_shape = (160, 576)\n",
    "weights_initializer_stddev = 0.01\n",
    "weights_regularized_l2 = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16\n",
    "\n",
    "\n",
    "First we're going to load VGG16 with pretrained weights (so it maintains its feature detectors, which can be useful for our smaller dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "#Download VGG16 if it is not already\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"vgg16.zip\"):\n",
    "    urlretrieve(\n",
    "        'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip',\n",
    "        \"./vgg16.zip\")\n",
    "    print(\"Downloaded VGG16 model weights\")\n",
    "else:\n",
    "    print(\"Already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping extraction\n"
     ]
    }
   ],
   "source": [
    "#Extract if needed\n",
    "if not os.path.exists(\"./vgg\"):\n",
    "    unzip = zipfile.ZipFile(\"./vgg16.zip\", \"r\")\n",
    "    unzip.extractall(\"./\")\n",
    "    unzip.close()\n",
    "    print(\"Extracted VGG16 model weights\")\n",
    "else:\n",
    "    print(\"Already exists, skipping extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a map more colorful\n",
    "def convertToColor(value):\n",
    "    colors = [\n",
    "        (255, 255, 255),   #0\n",
    "        (255, 0, 0),      #1\n",
    "        (0, 255, 0),      #2\n",
    "        (0, 0, 255),      #3\n",
    "        (255, 255, 0),    #4\n",
    "        (127, 0, 255),    #5\n",
    "        (51, 255, 51),    #6\n",
    "        (255, 0, 127),    #7\n",
    "        (127, 127, 127),  #8\n",
    "        (0, 0, 0),        #9\n",
    "        (0, 255, 255),  #10\n",
    "        (0, 0, 100),      #11\n",
    "        (100, 0, 0),      #12\n",
    "    ]\n",
    "    return colors[value[0]]\n",
    "\n",
    "def colorizeMap(img):\n",
    "    return [list( map(convertToColor, row) ) for row in img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to be able to convert the image map to a 13 channel ground truth map, and vice versa\n",
    "def pixelToTruth(value):\n",
    "    truths = [\n",
    "        (1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)\n",
    "    ]\n",
    "    \n",
    "    return truths[value[0]]\n",
    "\n",
    "def truthToPixel(value):\n",
    "    return (value.tolist().index(1), 0, 0)\n",
    "\n",
    "def imageToTruth(img):\n",
    "    return [list(map(pixelToTruth, row)) for row in img]\n",
    "\n",
    "def truthToImage(truth):\n",
    "    return [list(map(truthToPixel, row)) for row in truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the image data and the label for it\n",
    "def get_training_data(batch_size):\n",
    "    #Both inputs and ground truth maps have the same name - easy!\n",
    "    image_paths = glob(os.path.join(\"./data/Train/CameraRGB\", \"*.png\"))\n",
    "    label_paths = glob(os.path.join(\"./data/Train/CameraSeg\", \"*.png\"))\n",
    "    \n",
    "    for batch in range(0, len(image_paths), batch_size):\n",
    "        images = []\n",
    "        maps = []\n",
    "        \n",
    "        for index, image_file in enumerate(image_paths[batch:batch + batch_size]):\n",
    "#             map_file = os.path.join(\"./data/Train/CameraSeg\", label_paths[os.path.basename(image_file)])\n",
    "            map_file = os.path.join(label_paths[index])\n",
    "            \n",
    "            image = cv2.imread(image_file)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, image_shape)\n",
    "            map_image = cv2.imread(map_file)\n",
    "            map_image = cv2.cvtColor(map_image, cv2.COLOR_BGR2RGB)\n",
    "            map_image = cv2.resize(map_image, image_shape)\n",
    "            map_image = imageToTruth(map_image)\n",
    "            \n",
    "#             bg = np.all(map_image == np.array([0, 0, 0]), axis=2)\n",
    "#             bg = bg.reshape(*bg.shape, 1)\n",
    "#             map_image = np.concatenate((bg, np.invert(bg)), axis=2)\n",
    "            \n",
    "            images.append(image)\n",
    "            maps.append(map_image)\n",
    "\n",
    "        yield np.array(images), np.array(maps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./vgg/variables/variables'\n",
      "WARNING:tensorflow:From <ipython-input-10-0c02f8e06f87>:72: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Graph has been built - launching training\n",
      "====== :-) ======\n",
      "Launching Epoch 0\n",
      "Batch 10 - loss of 2.564948797225952\n",
      "Batch 20 - loss of 2.5649497509002686\n",
      "Batch 30 - loss of 2.564948558807373\n",
      "Batch 40 - loss of 2.5649499893188477\n",
      "Batch 50 - loss of 2.5649471282958984\n",
      "Batch 60 - loss of 2.5649476051330566\n",
      "Batch 70 - loss of 2.5649492740631104\n",
      "Batch 80 - loss of 2.5649495124816895\n",
      "Batch 90 - loss of 2.564948797225952\n",
      "Batch 100 - loss of 2.5649492740631104\n",
      "Batch 110 - loss of 2.564948081970215\n",
      "Batch 120 - loss of 2.564948320388794\n",
      "Batch 130 - loss of 2.564948797225952\n",
      "Batch 140 - loss of 2.5649492740631104\n",
      "Batch 150 - loss of 2.5649468898773193\n",
      "Batch 160 - loss of 2.5649497509002686\n",
      "Batch 170 - loss of 2.564948320388794\n",
      "Batch 180 - loss of 2.564948797225952\n",
      "Batch 190 - loss of 2.5649492740631104\n",
      "Batch 200 - loss of 2.564948797225952\n",
      "Batch 210 - loss of 2.564948797225952\n",
      "Batch 220 - loss of 2.5649490356445312\n",
      "Batch 230 - loss of 2.564948797225952\n",
      "Batch 240 - loss of 2.5649490356445312\n",
      "Batch 250 - loss of 2.564948558807373\n",
      "Batch 260 - loss of 2.564948558807373\n",
      "Batch 270 - loss of 2.564948081970215\n",
      "Batch 280 - loss of 2.564948558807373\n",
      "Batch 290 - loss of 2.5649478435516357\n",
      "Batch 300 - loss of 2.564948797225952\n",
      "Batch 310 - loss of 2.5649476051330566\n",
      "Batch 320 - loss of 2.564946413040161\n",
      "Batch 330 - loss of 2.564948081970215\n",
      "Batch 340 - loss of 2.5649495124816895\n",
      "Batch 350 - loss of 2.564948797225952\n",
      "Batch 360 - loss of 2.5649495124816895\n",
      "Batch 370 - loss of 2.5649468898773193\n",
      "Batch 380 - loss of 2.5649492740631104\n",
      "Batch 390 - loss of 2.5649502277374268\n",
      "Batch 400 - loss of 2.5649476051330566\n",
      "Batch 410 - loss of 2.564948797225952\n",
      "Batch 420 - loss of 2.564948797225952\n",
      "Batch 430 - loss of 2.564948558807373\n",
      "Batch 440 - loss of 2.5649499893188477\n",
      "Batch 450 - loss of 2.564951181411743\n",
      "Batch 460 - loss of 2.564948797225952\n",
      "Batch 470 - loss of 2.564948558807373\n",
      "Batch 480 - loss of 2.564948797225952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0c02f8e06f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m#get the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mbatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             _, loss = sess.run(\n",
      "\u001b[0;32m<ipython-input-7-f19592ddac72>\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mmap_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    #Placeholders\n",
    "    label = tf.placeholder(tf.int32, (None, None, None, num_classes), name='label')\n",
    "    learning_rate = tf.placeholder(tf.int32, name='learning_rate')\n",
    "    \n",
    "    #Grab layers from pretrained VGG\n",
    "    tf.saved_model.loader.load(sess, [\"vgg16\"], \"./vgg/\")\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #define key layers for us to work with, so we can take pieces of VGG16\n",
    "    #for our own use\n",
    "    input_layer = graph.get_tensor_by_name(\"image_input:0\")\n",
    "    keep_prob = graph.get_tensor_by_name(\"keep_prob:0\") #Dropout settings\n",
    "    \n",
    "    #more layer grabbing\n",
    "    layer3 = graph.get_tensor_by_name(\"layer3_out:0\")\n",
    "    layer4 = graph.get_tensor_by_name(\"layer4_out:0\")\n",
    "    layer7 = graph.get_tensor_by_name(\"layer7_out:0\")\n",
    "    \n",
    "    #Create new output layers\n",
    "    #First, a 1x1 convolutional to maintain spacial data\n",
    "    layer_8_conv = tf.layers.conv2d(layer7, num_classes, 1,\n",
    "                                padding='same', name='layer_8_conv',\n",
    "                                kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #transpose by 2 for then ext layer\n",
    "    layer_9_transpose = tf.layers.conv2d_transpose(layer_8_conv, num_classes,\n",
    "                                4, strides=2, padding='same',\n",
    "                                name='layer_9_transpose',\n",
    "                                kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #Another convolution\n",
    "    layer_10_conv = tf.layers.conv2d(layer_9_transpose, num_classes, 1,\n",
    "                                padding='same', name='layer_10_conv',\n",
    "                                kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #Skip layer - so we dont lose too much positional information during conv/transposeds\n",
    "    layer_11_skip = tf.add(layer_9_transpose, layer_10_conv, name='layer_11_skip')\n",
    "    \n",
    "    #transpose again\n",
    "    layer_12_transpose = tf.layers.conv2d_transpose(layer_11_skip, num_classes,\n",
    "                                4, strides=2, padding='same',\n",
    "                                name='layer_12_transpose',\n",
    "                                kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #and convolve...\n",
    "    layer_13_conv = tf.layers.conv2d(layer_12_transpose, num_classes, 1,\n",
    "                                padding='same', name='layer_13_conv',\n",
    "                                kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #skip again\n",
    "    layer_14_skip = tf.add(layer_12_transpose, layer_13_conv, name='layer_14_skip')\n",
    "    \n",
    "    #Transpose\n",
    "    output_layer = tf.layers.conv2d_transpose(layer_14_skip, num_classes, 16, strides=8,\n",
    "                                padding='same', name='output_layer',\n",
    "                                kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #layer_15 will be our transposed output!\n",
    "    \n",
    "    #Create the optimzer\n",
    "    logits = tf.reshape(output_layer, (-1, num_classes))\n",
    "    correct_label = tf.reshape(label, (-1, num_classes))\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label))\n",
    "    optimizer= tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy_loss)\n",
    "    \n",
    "    \n",
    "    #and now, training!\n",
    "    epochs = 1\n",
    "    batch_size = 1\n",
    "    keep_probability = 0.5\n",
    "    learning_rate_alpha = 0.001\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Graph has been built - launching training\")\n",
    "    print(\"====== :-) ======\")\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Launching Epoch {}\".format(epoch))\n",
    "        loss_log = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        #get the images\n",
    "        for image, truth in get_training_data(batch_size):\n",
    "            batch_count += 1\n",
    "            _, loss = sess.run(\n",
    "                    [train_op, cross_entropy_loss],\n",
    "                    feed_dict = {\n",
    "                        input_layer: image,\n",
    "                        label: truth,\n",
    "                        keep_prob: keep_probability,\n",
    "                        learning_rate: learning_rate_alpha\n",
    "                    }\n",
    "                )\n",
    "            loss_log.append('{:3f}'.format(loss))\n",
    "            if(batch_count % 10 == 0):\n",
    "                print(\"Batch {} - loss of {}\".format(batch_count, loss))\n",
    "        print(\"Training for epoch finished - \", loss_log[-1])\n",
    "        print()\n",
    "    print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
