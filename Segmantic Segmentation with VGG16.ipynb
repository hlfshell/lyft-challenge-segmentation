{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmantic Segmentation with VGG16\n",
    "\n",
    "The initial attempt with using a GAN hit a wall, specifically with figuring out how to do proper layer sizing for the transposed convolution layers for the generator. On top of that, some research I did during my \"off\" time showed that the approach semantic segmentation using either an FCN (Fully Convolutional Network) approach or SegNet would work best.\n",
    "\n",
    "Since SegNet is completely new, whereas transfer learning with VGG16 is something I'm familiar with, I'm going to attempt a VGG16 network here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n",
      "TensorFlow Version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from glob import glob\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "#Check GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful variables\n",
    "num_classes = 13 # none and 12 options, 0-12\n",
    "image_shape = (160, 576)\n",
    "weights_initializer_stddev = 0.01\n",
    "weights_regularized_l2 = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16\n",
    "\n",
    "\n",
    "First we're going to load VGG16 with pretrained weights (so it maintains its feature detectors, which can be useful for our smaller dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "#Download VGG16 if it is not already\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"vgg16.zip\"):\n",
    "    urlretrieve(\n",
    "        'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip',\n",
    "        \"./vgg16.zip\")\n",
    "    print(\"Downloaded VGG16 model weights\")\n",
    "else:\n",
    "    print(\"Already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping extraction\n"
     ]
    }
   ],
   "source": [
    "#Extract if needed\n",
    "if not os.path.exists(\"./vgg\"):\n",
    "    unzip = zipfile.ZipFile(\"./vgg16.zip\", \"r\")\n",
    "    unzip.extractall(\"./\")\n",
    "    unzip.close()\n",
    "    print(\"Extracted VGG16 model weights\")\n",
    "else:\n",
    "    print(\"Already exists, skipping extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a map more colorful\n",
    "def convertToColor(value):\n",
    "    colors = [\n",
    "        (255, 255, 255),   #0\n",
    "        (255, 0, 0),      #1\n",
    "        (0, 255, 0),      #2\n",
    "        (0, 0, 255),      #3\n",
    "        (255, 255, 0),    #4\n",
    "        (127, 0, 255),    #5\n",
    "        (51, 255, 51),    #6\n",
    "        (255, 0, 127),    #7\n",
    "        (127, 127, 127),  #8\n",
    "        (0, 0, 0),        #9\n",
    "        (0, 255, 255),  #10\n",
    "        (0, 0, 100),      #11\n",
    "        (100, 0, 0),      #12\n",
    "    ]\n",
    "    return colors[value[0]]\n",
    "\n",
    "def colorizeMap(img):\n",
    "    return [list( map(convertToColor, row) ) for row in img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to be able to convert the image map to a 13 channel ground truth map, and vice versa\n",
    "def pixelToTruth(value):\n",
    "    truths = [\n",
    "        (1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)\n",
    "    ]\n",
    "    \n",
    "    return truths[value[0]]\n",
    "\n",
    "def truthToPixel(value):\n",
    "    return (value.tolist().index(1), 0, 0)\n",
    "\n",
    "def imageToTruth(img):\n",
    "    return [list(map(pixelToTruth, row)) for row in img]\n",
    "\n",
    "def truthToImage(truth):\n",
    "    return [list(map(truthToPixel, row)) for row in truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the image data and the label for it\n",
    "def get_training_data(batch_size):\n",
    "    #Both inputs and ground truth maps have the same name - easy!\n",
    "    image_paths = glob(os.path.join(\"./data/Train/CameraRGB\", \"*.png\"))\n",
    "    label_paths = glob(os.path.join(\"./data/Train/CameraSeg\", \"*.png\"))\n",
    "    \n",
    "    chosen_image = '165.png';\n",
    "    \n",
    "    for batch in range(0, len(image_paths), batch_size):\n",
    "        images = []\n",
    "        maps = []\n",
    "        \n",
    "#         for image_file in image_paths[batch:batch + batch_size]:\n",
    "#             image = cv2.imread(\"./data/Train/CameraRGB/{}\".format(chosen_image))\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             image = cv2.resize(image, image_shape)\n",
    "#             map_image = cv2.imread(\"./data/Train/CameraSeg/{}\".format(chosen_image))\n",
    "#             map_image = cv2.cvtColor(map_image, cv2.COLOR_BGR2RGB)\n",
    "#             map_image = cv2.resize(map_image, image_shape)\n",
    "#             map_image = imageToTruth(map_image)\n",
    "            \n",
    "#             images.append(image)\n",
    "#             maps.append(map_image)\n",
    "            \n",
    "#         yield np.array(images), np.array(maps)\n",
    "    \n",
    "    for batch in range(0, len(image_paths), batch_size):\n",
    "        images = []\n",
    "        maps = []\n",
    "        \n",
    "        for index, image_file in enumerate(image_paths[batch:batch + batch_size]):\n",
    "            map_file = os.path.join(label_paths[index])\n",
    "            \n",
    "            image = cv2.imread(image_file)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, image_shape)\n",
    "            map_image = cv2.imread(map_file)\n",
    "            map_image = cv2.cvtColor(map_image, cv2.COLOR_BGR2RGB)\n",
    "            map_image = cv2.resize(map_image, image_shape)\n",
    "            map_image = imageToTruth(map_image)\n",
    "            \n",
    "            images.append(image)\n",
    "            maps.append(map_image)\n",
    "\n",
    "        yield np.array(images), np.array(maps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./vgg/variables/variables'\n",
      "Graph has been built - launching training\n",
      "====== :-) ======\n",
      "\n",
      "Launching Epoch 0\n",
      "(?, ?, ?, 13)\n",
      "(?, ?, ?, 13)\n",
      "(?, ?, ?, 13)\n",
      "(?, ?, ?, 13)\n",
      "(?, ?, ?, 13)\n",
      "(?, ?, ?, 13)\n",
      "(?, ?, ?, 13)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-373985be9023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#get the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mbatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             _, loss = sess.run(\n",
      "\u001b[0;32m<ipython-input-8-56f2f0b938d7>\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mmaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    #Placeholders\n",
    "    label = tf.placeholder(tf.int32, (None, None, None, num_classes), name='label')\n",
    "    learning_rate = tf.placeholder(tf.int32, name='learning_rate')\n",
    "    \n",
    "    #Grab layers from pretrained VGG\n",
    "    tf.saved_model.loader.load(sess, [\"vgg16\"], \"./vgg/\")\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #define key layers for us to work with, so we can take pieces of VGG16\n",
    "    #for our own use\n",
    "    input_layer = graph.get_tensor_by_name(\"image_input:0\")\n",
    "    keep_prob = graph.get_tensor_by_name(\"keep_prob:0\") #Dropout settings\n",
    "    \n",
    "    #more layer grabbing\n",
    "    layer_3 = graph.get_tensor_by_name(\"layer3_out:0\")\n",
    "    layer_4 = graph.get_tensor_by_name(\"layer4_out:0\")\n",
    "    layer_7 = graph.get_tensor_by_name(\"layer7_out:0\")\n",
    "    \n",
    "#     kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "#     kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2)\n",
    "\n",
    "\n",
    "    # Skip connections for later\n",
    "    skip_conv_3 = tf.layers.conv2d(layer_3, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    skip_conv_4 = tf.layers.conv2d(layer_4, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #Layer 7 isn't skipped, it's passed right to transpose    \n",
    "    fully_connected_convs = tf.layers.conv2d(layer_7, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #From layer 7 we need to transpose up\n",
    "    transpose_1 = tf.layers.conv2d_transpose(fully_connected_convs, num_classes, 4, 2, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    # Add the skip layer from layer 4\n",
    "    skip_1 = tf.add(transpose_1, skip_conv_4)\n",
    "    \n",
    "    #Tranpose up from resultant layer\n",
    "    transpose_2 = tf.layers.conv2d_transpose(skip_1, num_classes, 4, 2, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #Create skip layer from layer 3\n",
    "    skip_2 = tf.add(skip_conv_3, transpose_2)\n",
    "    \n",
    "    #Final output layer\n",
    "    output_layer = tf.layers.conv2d_transpose(skip_2, num_classes, 16, 8, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "            activation=tf.sigmoid)\n",
    "    \n",
    "    #layer_15 will be our transposed output!\n",
    "    \n",
    "    #Create the optimzer\n",
    "    logits = tf.reshape(output_layer, (-1, num_classes))\n",
    "    correct_label = tf.reshape(label, (-1, num_classes))\n",
    "#     mean_absolute = tf.metrics.mean_absolute_error(tf.cast(logits, tf.float32), tf.cast(correct_label, tf.float32))\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label))\n",
    "    optimizer= tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy_loss)\n",
    "#     train_op = optimizer.minimize(mean_absolute)\n",
    "    \n",
    "    \n",
    "    #and now, training!\n",
    "    epochs = 1\n",
    "    batch_size = 10\n",
    "    keep_probability = 0.5\n",
    "    learning_rate_alpha = 0.0001\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Graph has been built - launching training\")\n",
    "    print(\"====== :-) ======\")\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Launching Epoch {}\".format(epoch))\n",
    "        loss_log = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        #get the images\n",
    "        for image, truth in get_training_data(batch_size):\n",
    "            batch_count += 1\n",
    "            _, loss = sess.run(\n",
    "                    [train_op, cross_entropy_loss],\n",
    "                    feed_dict = {\n",
    "                        input_layer: image,\n",
    "                        label: truth,\n",
    "                        keep_prob: keep_probability,\n",
    "                        learning_rate: learning_rate_alpha\n",
    "                    }\n",
    "                )\n",
    "            loss_log.append('{:3f}'.format(loss))\n",
    "            if(batch_count % 10 == 0):\n",
    "                print(\"Batch {} - loss of {}\".format(batch_count, loss))\n",
    "        print(\"Training for epoch finished - \", loss_log[-1])\n",
    "        print()\n",
    "    print(\"Training finished\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
