{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmantic Segmentation with VGG16\n",
    "\n",
    "The initial attempt with using a GAN hit a wall, specifically with figuring out how to do proper layer sizing for the transposed convolution layers for the generator. On top of that, some research I did during my \"off\" time showed that the approach semantic segmentation using either an FCN (Fully Convolutional Network) approach or SegNet would work best.\n",
    "\n",
    "Since SegNet is completely new, whereas transfer learning with VGG16 is something I'm familiar with, I'm going to attempt a VGG16 network here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n",
      "TensorFlow Version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from glob import glob\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "#Check GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful variables\n",
    "num_classes = 13 # none and 12 options, 0-12\n",
    "image_shape = (160, 576)\n",
    "weights_initializer_stddev = 0.01\n",
    "weights_regularized_l2 = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16\n",
    "\n",
    "\n",
    "First we're going to load VGG16 with pretrained weights (so it maintains its feature detectors, which can be useful for our smaller dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "#Download VGG16 if it is not already\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"vgg16.zip\"):\n",
    "    urlretrieve(\n",
    "        'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip',\n",
    "        \"./vgg16.zip\")\n",
    "    print(\"Downloaded VGG16 model weights\")\n",
    "else:\n",
    "    print(\"Already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists, skipping extraction\n"
     ]
    }
   ],
   "source": [
    "#Extract if needed\n",
    "if not os.path.exists(\"./vgg\"):\n",
    "    unzip = zipfile.ZipFile(\"./vgg16.zip\", \"r\")\n",
    "    unzip.extractall(\"./\")\n",
    "    unzip.close()\n",
    "    print(\"Extracted VGG16 model weights\")\n",
    "else:\n",
    "    print(\"Already exists, skipping extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a map more colorful\n",
    "def convertToColor(value):\n",
    "    colors = [\n",
    "        (255, 255, 255),   #0\n",
    "        (255, 0, 0),      #1\n",
    "        (0, 255, 0),      #2\n",
    "        (0, 0, 255),      #3\n",
    "        (255, 255, 0),    #4\n",
    "        (127, 0, 255),    #5\n",
    "        (51, 255, 51),    #6\n",
    "        (255, 0, 127),    #7\n",
    "        (127, 127, 127),  #8\n",
    "        (0, 0, 0),        #9\n",
    "        (0, 255, 255),  #10\n",
    "        (0, 0, 100),      #11\n",
    "        (100, 0, 0),      #12\n",
    "    ]\n",
    "    return colors[value[0]]\n",
    "\n",
    "def colorizeMap(img):\n",
    "    return [list( map(convertToColor, row) ) for row in img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to be able to convert the image map to a 13 channel ground truth map, and vice versa\n",
    "def pixelToTruth(value):\n",
    "    truths = [\n",
    "        (1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0),\n",
    "        (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)\n",
    "    ]\n",
    "    \n",
    "    return truths[value[0]]\n",
    "\n",
    "def truthToPixel(value):\n",
    "    return (value.tolist().index(1), 0, 0)\n",
    "\n",
    "def imageToTruth(img):\n",
    "    return [list(map(pixelToTruth, row)) for row in img]\n",
    "\n",
    "def truthToImage(truth):\n",
    "    return [list(map(truthToPixel, row)) for row in truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the image data and the label for it\n",
    "def get_training_data(batch_size):\n",
    "    #Both inputs and ground truth maps have the same name - easy!\n",
    "    image_paths = glob(os.path.join(\"./data/Train/CameraRGB\", \"*.png\"))\n",
    "    label_paths = glob(os.path.join(\"./data/Train/CameraSeg\", \"*.png\"))\n",
    "    \n",
    "    chosen_image = '165.png';\n",
    "    \n",
    "    for batch in range(0, len(image_paths), batch_size):\n",
    "        images = []\n",
    "        maps = []\n",
    "        \n",
    "#         for image_file in image_paths[batch:batch + batch_size]:\n",
    "#             image = cv2.imread(\"./data/Train/CameraRGB/{}\".format(chosen_image))\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             image = cv2.resize(image, image_shape)\n",
    "#             map_image = cv2.imread(\"./data/Train/CameraSeg/{}\".format(chosen_image))\n",
    "#             map_image = cv2.cvtColor(map_image, cv2.COLOR_BGR2RGB)\n",
    "#             map_image = cv2.resize(map_image, image_shape)\n",
    "#             map_image = imageToTruth(map_image)\n",
    "            \n",
    "#             images.append(image)\n",
    "#             maps.append(map_image)\n",
    "            \n",
    "#         yield np.array(images), np.array(maps)\n",
    "    \n",
    "    for batch in range(0, len(image_paths), batch_size):\n",
    "        images = []\n",
    "        maps = []\n",
    "        \n",
    "        for index, image_file in enumerate(image_paths[batch:batch + batch_size]):\n",
    "            map_file = os.path.join(label_paths[index])\n",
    "            \n",
    "            image = cv2.imread(image_file)\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, image_shape)\n",
    "#             image = image - np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "            map_image = cv2.imread(map_file)\n",
    "            map_image = cv2.cvtColor(map_image, cv2.COLOR_BGR2RGB)\n",
    "            map_image = cv2.resize(map_image, image_shape)\n",
    "            map_image = imageToTruth(map_image)\n",
    "            \n",
    "            images.append(image)\n",
    "            maps.append(map_image)\n",
    "\n",
    "        yield np.array(images), np.array(maps)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./vgg/variables/variables'\n",
      "Graph has been built - launching training\n",
      "====== :-) ======\n",
      "\n",
      "Launching Epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4cdea314e852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                         \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                     }\n\u001b[1;32m    104\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    #Placeholders\n",
    "    label = tf.placeholder(tf.int32, (None, None, None, num_classes), name='label')\n",
    "    learning_rate = tf.placeholder(tf.int32, name='learning_rate')\n",
    "    \n",
    "    #Grab layers from pretrained VGG\n",
    "    tf.saved_model.loader.load(sess, [\"vgg16\"], \"./vgg/\")\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #define key layers for us to work with, so we can take pieces of VGG16\n",
    "    #for our own use\n",
    "    input_layer = graph.get_tensor_by_name(\"image_input:0\")\n",
    "    keep_prob = graph.get_tensor_by_name(\"keep_prob:0\") #Dropout settings\n",
    "    \n",
    "    #more layer grabbing\n",
    "    layer_3 = graph.get_tensor_by_name(\"layer3_out:0\")\n",
    "    layer_4 = graph.get_tensor_by_name(\"layer4_out:0\")\n",
    "    layer_7 = graph.get_tensor_by_name(\"layer7_out:0\")\n",
    "    \n",
    "#     kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "#     kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2)\n",
    "\n",
    "\n",
    "    # Skip connections for later\n",
    "    skip_conv_3 = tf.layers.conv2d(layer_3, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    skip_conv_4 = tf.layers.conv2d(layer_4, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #Layer 7 isn't skipped, it's passed right to transpose    \n",
    "    fully_connected_convs = tf.layers.conv2d(layer_7, num_classes, 1, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #From layer 7 we need to transpose up\n",
    "    transpose_1 = tf.layers.conv2d_transpose(fully_connected_convs, num_classes, 4, 2, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    # Add the skip layer from layer 4\n",
    "    skip_1 = tf.add(transpose_1, skip_conv_4)\n",
    "    \n",
    "    #Tranpose up from resultant layer\n",
    "    transpose_2 = tf.layers.conv2d_transpose(skip_1, num_classes, 4, 2, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2))\n",
    "    \n",
    "    #Create skip layer from layer 3\n",
    "    skip_2 = tf.add(skip_conv_3, transpose_2)\n",
    "    \n",
    "    #Final output layer\n",
    "    output_layer = tf.layers.conv2d_transpose(skip_2, num_classes, 16, 8, padding='same',\n",
    "            kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "            kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "            activation=tf.sigmoid)\n",
    "    \n",
    "    #layer_15 will be our transposed output!\n",
    "    \n",
    "    #Create the optimzer\n",
    "    logits = tf.reshape(output_layer, (-1, num_classes))\n",
    "    correct_label = tf.reshape(label, (-1, num_classes))\n",
    "#     mean_absolute = tf.metrics.mean_absolute_error(tf.cast(logits, tf.float32), tf.cast(correct_label, tf.float32))\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label))\n",
    "    optimizer= tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy_loss)\n",
    "#     train_op = optimizer.minimize(mean_absolute)\n",
    "    \n",
    "    \n",
    "    #and now, training!\n",
    "    epochs = 1\n",
    "    batch_size = 10\n",
    "    keep_probability = 0.5\n",
    "    learning_rate_alpha = 0.0001\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Graph has been built - launching training\")\n",
    "    print(\"====== :-) ======\")\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Launching Epoch {}\".format(epoch))\n",
    "        loss_log = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        #get the images\n",
    "        for image, truth in get_training_data(batch_size):\n",
    "            batch_count += 1\n",
    "            _, loss = sess.run(\n",
    "                    [train_op, cross_entropy_loss],\n",
    "                    feed_dict = {\n",
    "                        input_layer: image,\n",
    "                        label: truth,\n",
    "                        keep_prob: keep_probability,\n",
    "                        learning_rate: learning_rate_alpha\n",
    "                    }\n",
    "                )\n",
    "            loss_log.append('{:3f}'.format(loss))\n",
    "            if(batch_count % 10 == 0):\n",
    "                print(\"Batch {} - loss of {}\".format(batch_count, loss))\n",
    "        print(\"Training for epoch finished - \", loss_log[-1])\n",
    "        print()\n",
    "    print(\"Training finished\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
